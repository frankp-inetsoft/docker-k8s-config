= Container Configuration
InetSoft Technology <info@inetsoft.com>
v2021
:doctype: article
:icons: font
:source-highlighter: highlightjs
:toc: left
:tocLevels: 3
:sectlinks:
:imagesdir: images
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

[[overview]]
== Overview

Container environment variables and an external source of configuration files and scripts are used to configure the containers.

NOTE: All configuration is done on the initialization containers. The only exception is the `INETSOFT_MASTER_PASSWORD` environment variable, which should be set on the initialization, scheduler, and server containers.

[[variables]]
== Configuration Variables

The following variables are used to configure the software:

|===
| Variable | Description | Default Value

| `INETSOFT_MASTER_PASSWORD`
| The master encryption password.
| _None_

| `INETSOFT_MASTER_SALT`
| The master password salt. Only used in FIPS mode.
| _None_

| `INETSOFT_FIPS`
| Enables FIPS mode if `true`.
| _None_

| `INETSOFT_ADMIN_USER`
| The default admin username.
| `admin`

| `INETSOFT_ADMIN_PASSWORD`
| The default admin password.
| `admin`

| `INETSOFT_LICENSE`
| The license key(s).
| _None_

| `INETSOFT_SCHEDULER_LICENSE`
| The scheduler license keys(s).
| _None_

| `INETSOFT_DB_TYPE`
| The type of internal database. Must be one of `SQLServer`, `Oracle`,   `Derby`, `DB2`, `PostgreSQL`, `MySQL`, or `H2`.
| `H2`

| `INETSOFT_DB_URL`
| The JDBC URL for the internal database.
| `jdbc:h2:/var/lib/inetsoft/shared/data/inetsoftdb;...`

| `INETSOFT_DB_DRIVER`
| The JDBC driver class for internal database.
| `org.h2.Driver`

| `INETSOFT_DB_USER`
| The username for the internal database.
| `inetsoft_admin`

| `INETSOFT_DB_PASSWORD`
| The password for the internal database.
| `1Secret!`

| `INETSOFT_CLUSTERED`
| Enables the cluster server mode if `true`.
| `false`

| `INETSOFT_PROXY_URL`
| The URL of the proxy server.
| _None_

| `INETSOFT_K8S_API_SERVER`
| The hostname and port of the Kuberentes API server.
| `kubernetes.default.svc`

| `INETSOFT_K8S_TOKEN`
| The authentication token for the Kubernetes API server.
| Read from the `/var/run/secrets/kubernetes.io/serviceaccount/token` file.

| `INETSOFT_K8S_NAMESPACE`
| The Kubernetes namespace into which the container is deployed.
| Read from the `/var/run/secrets/kubernetes.io/serviceaccount/namespace` file.

| `INETSOFT_K8S_LABEL_NAME`
| The name of the label used to identify InetSoft server and scheduler pods.
| _None_

| `INETSOFT_K8S_LABEL_VALUE`
| The value of the label used to identify InetSoft server and scheduler pods.
| _None_
|===

[[external-config]]
== External Configuration

External configuration can be supplied by mounting an external volume containing the configuration at `/var/lib/inetsoft/external`, or by setting environment variables that control where the configuration is copied from.

The external configuration can be copied from a Git repository or any location supported by https://commons.apache.org/proper/commons-vfs/filesystems.html[Apache Commons VFS].

The environment variables specifying the location are as follows:

|===
| Variable | Description

| `INETSOFT_CONFIG_URL`
| The URL of the configuration.

| `INETSOFT_CONFIG_USERNAME`
| The username for the configuration URL.

| `INETSOFT_CONFIG_PASSWORD`
| The password for the configuration URL.

| `INETSOFT_CONFIG_KEYFILE`
| The location of the SSH key file. If specified, `INETSOFT_CONFIG_PASSWORD` should be the password for the key file, if required.

| `INETSOFT_CONFIG_BRANCH`
| The branch or tag if using the Git repository.

| `INETSOFT_CONFIG_PATH`
| The path, relative to the URL, containing the configuration. If not specified, the URL will be used as the base of the configuration.

| `INETSOFT_CONFIG_SFTP_USEROOTDIR`
| If `true`, use the root directory instead of the user's home directory when using SFTP. By default, the user's home directory will be used.
|===

The URL for Git repositories should be prefixed with `git://`, for example, `git://http://host/...`, `git://https://host/...`, or `git://ssh://user@host:/...`.

The URL may be for a directory containing the external configuration or an archive file containing the external configuration. Any archive file format supported by https://commons.apache.org/proper/commons-compress/[Apache Commons Compress] may be used, including GZIPed archives of supported formats (e.g. `*.tar.gz`).

HTTP and HTTPS do not support directory listing, so if using one of these protocols, it _must_ be for an archive file.

If you are mounting the external configuration to the `/var/lib/inetsoft/external` volume, the URL should be set to `file:///var/lib/inetsoft/external`. The `file:` protocol should not be used otherwise.

The external configuration may contain the following directories:

|===
| Directory | Description

| `assets/`
| Asset ZIP files that will be imported into the repository.

| `config/`
| Files to be placed in the data space. It may include an `asset.dat.d` directory containing assets. This is essentially a local `sree.home` directory.

| `drivers/`
| Additional JDBC drivers.

| `lib/`
| Additional JAR files that should be added to the application class path.

| `plugins/`
| Additional plugins.

| `scripts/`
| Additional or overridden initialization scripts.
|===

An example of an external configuration can be found in the `config/`
directory of this repository.

[[scripts]]
=== Initialization Scripts

Initialization scripts are shell (`.sh`) or Groovy (`.groovy`) scripts that are named using a convention that will ensure the order of their execution. For example, `00-start.sh` would be executed first and `99-finish.groovy` would be executed last.

The builtin script that copies the files from the staging directory to the shared directory or database is named `50-stage-type.groovy`. That way, any scripts that should be executed before files are deployed into the data space should be less than 50 and any scripts that should be executed after they are deployed should be greater than 50. Groovy scripts should not call `connect` unless they are greater than 50.

The following script levels are reserved by pre-defined scripts:

* `00` - initializes the base properties and passwords.
* `49` - stages the shared files that are outside the data space, e.g. plugins and drivers.
* `50` - installs files from staging into the data space.
* `51` - re-encrypts the admin password to ensure FIPS compliance.
* `75` - imports all assets from `staging/assets` into the data space.

This convention allows external configurations to customize the configuration during various phases of the initialization process. For example, a script named `01-remove-extras.sh` could delete unwanted drivers or plugins from the staging directory. A script named `76-set-passwords.groovy` could change the username and password of a data source.

[[oak]]
=== inetsoft.yaml Configuration

The configuration of the storage back end is done in inetsoft.yaml. This file also contains the audit database configuration, the Hazelcast cluster configuration, as well a couple of other top-level initialization settings.

===== Shared Configuration
All configuration files share the following configuration settings, regardless of the storage implementation. These settings can be directly mapped to those in the legacy dbProp.properties file.

.inetsoft.yaml
[source,yaml]
----
version: "13.5.0"
audit:
  type: "H2"
  jdbcUrl: "jdbc:h2:/data_dir/inetsoftdb/inetsoftdb;MODE=Derby;AUTO_SERVER=TRUE;AUTO_SERVER_PORT=8192;AUTO_RECONNECT=TRUE"
  driverClassName: "org.h2.Driver"
  requiresLogin: true
  username: "inetsoft_admin"
  password: "\\masterWer7z5uT6zrmE3tVlyk4jc2dMBPKkJ4llCGAdNm2Pc9rLXDZAkZlW31X0zX2DRBeWDA="
  transactionIsolationLevel: "READ_UNCOMMITTED"
cluster:
  groupName: "inetsoft-f2767cbb-10f8-4495-8471-4bd9c4ba1414"
  groupPassword: "\\masterWNLYCLnP+I1D+HCVRYDFM7D5kF4RyTkHWVdUh2extYnODn6z1FWUmGrkP3wZcIgsCp93WXJabgR5H0D1HTvpZgvXkkyK0fLkVbPlptrm+uMk9jkh8w8="
  portNumber: 5701
  outboundPortNumber: 0
  multicastEnabled: true
  multicastAddress: "224.2.2.3"
  multicastPort: 54327
  tcpEnabled: false
pluginDirectories:
  - "/data_dir/plugins"
driverDirectories:
  - "/data_dir/drivers"
fipsComplianceMode: false
----
===== Cluster Configuration
Clusters *cannot* use the MapDB key-value storage or the local filesystem for blob storage. They should configure the cluster.quorumMembers property in inetsoft.yaml.

The quorum members contain the identifiers of the server and scheduler instances that are members of the cluster quorum. For any node in the cluster, at least (n / 2) + 1 quorum members must be visible to a node in order for that node to make any persistent changes. This prevents inconsistencies from being introduced during a split-brain condition due to network partitioning. The quorum member identifier for a server or scheduler instance is set by using the inetsoft.cluster.quorumMemberId system property when starting the instance.

For statically provisioned clusters, all server and scheduler nodes should be included in the quorum.

For elastic clusters, the "base" server and scheduler nodes should be included in the quorum. For example, if the minimum number of server nodes is configured to be 3 in a Kubernetes stateful set, the hostnames of the first three server pods and the hostnames for all the scheduler nodes should be included in the quorum. During normal operation these pods will always be running and provide a consistent cluster quorum.

For example, a statically provisioned cluster with 3 server instances and 1 scheduler instance should be configured using:

.inetsoft.yaml
[source,yaml]
cluster:
  quorumMembers:
    - server1
    - server2
    - server3
    - scheduler

The instances would be started with the following command line options, respectively:

[source]
-Dinetsoft.cluster.quorumMemberId=server1
-Dinetsoft.cluster.quorumMemberId=server2
-Dinetsoft.cluster.quorumMemberId=server3
-Dinetsoft.cluster.quorumMemberId=scheduler

A Kubernetes deployment a minimum number of server pods of 3, 1 scheduler pod and the name of the server stateful set server in the namespace inetsoft would use the configuration:

.inetsoft.yaml
[source,yaml]
cluster:
  quorumMembers:
    - server-0.inetsoft.svc.cluster.local
    - server-1.inetsoft.svc.cluster.local
    - server-2.inetsoft.svc.cluster.local
    - scheduler

The server pods would have the following command line option:
[source]
-Dinetsoft.cluster.quorumMemberId=$(hostname)

The scheduler pod would have the following command line option:
[source]
-Dinetsoft.cluster.quorumMemberId=scheduler

==== Configure Asset Storage
By default, inetsoft.yaml is configured using the internal database for the document node store and a file blob store with sensible file paths. If you want to use a different storage system, you'll need to include a custom `config/inetsoft.yaml` file in your external configuration.

The inetsoft.yaml asset storage configuration has the following structure:

.inetsoft.yaml
[source,yaml]
----
#Default
keyValue:
  type: "mapdb" # <1>
  mapdb: # <2>
    directory: "/data_dir/kv" # <3>
blob: # <4>
  type: "local" # <5>
  filesystem: # <6>
    directory: "/data_dir/blob" # <7>
#AWS
keyValue:
  type: dynamodb
  dynamodb: # <8>
    region: us-east-1 # <9>
    accessKeyId: your_access_key # <10>
    secretAccessKey: your_secret_key # <11>
    table: inetsoft # <12>
    provisionedReadThroughput: 300 # <13>
    provisionedWriteThroughput: 300 # <14>
blob:
  type: s3
  cacheDirectory: "/cachedir/blob" # <15>
  s3: # <16>
    region: us-east-1 # <17>
    accessKeyId: your_access_key # <18>
    secretAccessKey: your_secret_key # <19>
    bucket: inetsoft # <20>
#Azure
keyValue:
  type: "cosmosdb"
  cosmosdb: # <21>
    accountHost: "https://localhost:8081" # <22>
    accountKey: "your_account_key" # <23>
    database: "inetsoft" # <24>
    container: "inetsoft" # <25>
    throughput: 400 # <26>
blob:
  type: "azure"
  cacheDirectory: "/cache_dir/blob" # <27>
  azure: # <28>
    connectionString: "your_connection_string" # <29>
    container: "inetsoft" # <30>
#Google
keyValue:
  type: firestore
  firestore: # <31>
    serviceAccountFile: "/path/to/credentials.json" # <32>
    collection: inetsoft # <33>
blob:
  type: gcs
  cacheDirectory: "/cache_dir/blob" # <34>
  gcs: # <35>
    serviceAccountFile: "/path/to/credentials.json" # <36>
    bucket: inetsoft # <37>
#Mongo
keyValue:
  type: "mongo"
  mongo: # <38>
    hosts: # <39>
    - 'localhost:27017'
    database: test # <40>
    collection: inetsoft # <41>
#Relational Database (JDBC)
keyValue:
  type: "database"
  database: # <42>
    type: "H2" # <43>
    jdbcUrl: "jdbc:h2:/data_dir/kv/kv;MODE=Derby;AUTO_SERVER=TRUE;AUTO_SERVER_PORT=8192;AUTO_RECONNECT=TRUE" # <44>
    driverClassName: "org.h2.Driver" # <45>
    requiresLogin: true # <46>
    username: "inetsoft_admin" # <47>
    password: "\\masterWer7z5uT6zrmE3tVlyk4jc2dMBPKkJ4llCGAdNm2Pc9rLXDZAkZlW31X0zX2DRBeWDA=" # <48>
    transactionIsolationLevel: "READ_UNCOMMITTED" # <49>
----
<1> The Key-Value Storage type.
<2> The configuration for a MapDB key-value storage.
<3> The directory for a MapDB key-value storage.
<4> The configuration for the blob store.
<5> The type of blob store. NOTE: The local blob storage should only be used with a single server and single scheduler running on the same machine.
<6> Configuration for a file-based blob store.
<7> The directory where the blobs are created.
<8> Example configuration for an AWS cloud-native implementation. AWS uses DynamoDB for key-value storage and S3 for blob storage.
<9> The AWS region. If not specified, it will use the default region discovery of the AWS SDK.
<10> The AWS access key. If not specified, it will use the default credential discovery of the AWS SDK.
<11> The AWS secret key. If not specified, it will use the default credential discovery of the AWS SDK.
<12> The AWS DynamoDB table name.
<13> The AWS DynamoDB provisioned read throughput.
<14> The AWS DynamoDB provisioned write throughput.
<15> Configuration for the local file cache. Required.
<16> The configuration for an S3 blob store.
<17> The S3 region. If not specified, it will use the default region discovery of the AWS SDK.
<18> The AWS S3 access key. If not specified, it will use the default credential discovery of the AWS SDK.
<19> The AWS S3 secret key. If not specified, it will use the default credential discovery of the AWS SDK.
<20> The AWS S3 bucket name.
<21> Example configuration for Azure cloud-native implementation. Azure uses CosmosDB for key-value storage and Azure Blob for blob storage
<22> The CosmosDB  account host.
<23> The CosmosDB account key.
<24> The CosmosDB database.
<25> The CosmosDB container name.
<26> The CosmosDB throughput.
<27> The root directory of the blob cache. Required.
<28> The configuration for an Azure blob store.
<29> The Azure connection string.
<30> The Azure blob storage container name. Required if enabled.
<31> Example configuration for Google cloud-native implementation. Google uses Firestore for key-value storage and GCS for blob storage.
<32> The path to the Google Firestore Service Account File.
<33> The Google Firestore collection.
<34> The root directory of the blob cache. Required.
<35> The configuration for the GCS blob store.
<36> The path to the GCS Service Account File.
<37> The GCS bucket name.
<38> Example configuration for MongoDB key-value storage.
<39> The MongoDB hostname and ports.
<40> The name of the MongoDB database. Required if Mongo is used.
<41> The name of the MongoDB collection.
<42> Example configuration for a relational database (JDBC) key-value storage.
<43> The type of JDBC relational database.
<44> The URL of the JDBC relational database.
<45> The name of the driver class for the JDBC relational database.
<46> Set to true if JDBC relational database requires login.
<47> The username to log into the JDBC relational database. Required if requiresLogin is true.
<48> The password to log into the JDBC relational database. Required if requiresLogin is true.
<49> The JDBC relation database transaction isolation level.

For more information regarding the configuration of the asset storage, please see the documentation linked below:

https://www.inetsoft.com/docs/2023/userhelp/index.html#ProductDocs/administration/html/Configuring_the_Data_Spa.htm