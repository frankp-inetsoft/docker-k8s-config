= Deploying InetSoft in Docker and Kubernetes
InetSoft Technology <info@inetsoft.com>
v2021
:doctype: article
:icons: font
:source-highlighter: highlightjs
:toc: left
:tocLevels: 3
:sectlinks:
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

IMPORTANT: This is preliminary documentation for the upcoming 2021 release of the InetSoft software. This document is subject to change pending release.

[[overview]]
== Overview

InetSoft provides a set of images designed for use in Docker or deployment to a Kubernetes cluster. InetSoft only supports Docker and Kubernetes using these images. Customer-developed images will not be supported as part of the standard support contract.

[[quickstart]]
== Quickstart

Clone this repository to your local machine and change to its directory:

[source,shell]
----
git clone https://github.com/inetsoft-technology/docker-k8s-config.git
cd docker-k8s-config
----

[[quickstart-docker]]
=== Docker

Set the `INETSOFT_LICENSE` and `INETSOFT_MASTER_PASSWORD` environment variables:

[source,shell]
----
export INETSOFT_LICENSE=your_server_license
export INETSOFT_MASTER_PASSWORD=a_strong_password
----

Alternatively, you can create a `.env` file containing these variables:

..env
[source,properties]
----
INETSOFT_LICENSE=your_server_license
INETSOFT_MASTER_PASSWORD=a_strong_password
----

Finally, run the Docker Compose application:

[source,shell]
----
docker-compose up -d
----

The application will be accessible at http://localhost:8080/. The `shared` directory in the current directory will contain the persistent configuration files.

[[quickstart-kubernetes]]
=== Kubernetes

First, create a namespace for the application. The quickstart examples use the `inetsoft` namespace.

[source,shell]
----
kubectl create namespace inetsoft
----

Next, create a secret containing the credentials for the InetSoft Docker registry.

[source,shell]
----
kubectl -n inetsoft create secret docker-registry inetsoft-docker-secret \
  --docker-server=docker.inetsoft.com \
  --docker-username=your_email_address \
  --docker-password=a_valid_license_key
----

There are two types of example deployments: a single server deployment and a server cluster deployment.

[[quickstart-kubernetes-single]]
==== Single Server

The single server example deploys one scheduler and one server instance.

[[quickstart-kubernetes-single-config]]
===== Configuration

Create an overlay directory for the example deployment.

[source,shell]
----
mkdir -p overlays/quickstart
----

Create the Kustomize configuration for the overlay at `overlays/quickstart/kustomization.yaml` with the following content:

.overlays/quickstart/kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: inetsoft
bases:
  - ../../base
secretGenerator:
  - name: inetsoft
    behavior: merge
    envs:
      - secrets.properties
----

Override the master password and license key secrets with a unique password and your server license key, respectively. Put these in the `overlays/quickstart/secrets.properties` file.

.overlays/quickstart/secrets.properties
[source,properties]
----
INETSOFT_MASTER_PASSWORD=a_strong_password
INETSOFT_LICENSE=your_server_license_key
----

[[quickstart-kubernetes-single-pvc]]
===== Persistent Volume Provisioning

As configured, the example uses the `standard` storage class for Minikube. If your cluster uses a different storage class for dynamic provisioning, you'll need to override this in the `pvc-mysql.yaml` and `pvc-shared.yaml` files. For example, if you are using a NFS provisioner with the storage class `nfs-client`, create a file named `overlays/quickstart/pvc-mysql.yaml` with the following content:

.overlays/quickstart/pvc-mysql.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: inetsoft-mysql-pvc
spec:
  storageClassName: nfs-client
----

Then create a file named `overlays/quickstart/pvc-shared.yaml` with the following content:

.overlays/quickstart/pvc-shared.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: inetsoft-shared-pvc
spec:
  storageClassName: nfs-client
----

Then, append the following to `overlays/quickstart/kustomization.yaml`:

.overlays/quickstart/kustomization.yaml
[source,yaml]
----
patchesStrategicMerge:
  - pvc-mysql.yaml
  - pvc-shared.yaml
----

[[quickstart-kubernetes-single-deploy]]
===== Deployment

Deploy the application to Kubernetes by running:

[source,shell]
----
kubectl apply -k overlays/quickstart
----

Remove the application by running:

[source,shell]
----
kubectl delete -k overlays/quickstart
----

[[quickstart-kubernetes-cluster]]
==== Server Cluster

The cluster example deploys two server instances and an https://spark.apache.org/[Apache Spark] cluster with two worker nodes. This requires a pooled (concurrent session or named user license) or two CPU licenses for two cores each. In order to use two Spark worker nodes, you must provide a Spark license key for two worker nodes or reduce the number of worker nodes to one.

IMPORTANT: Minikube is insufficient to run this cluster deployment. You will need to deploy to a Kubernetes cluster with adequate CPU and memory resources available.

[[quickstart-kubernetes-cluster-config]]
===== Configuration

Create an overlay directory for the example deployment.

[source,shell]
----
mkdir -p overlays/quickstart
----

Create the Kustomize configuration for the overlay at `overlays/quickstart/kustomization.yaml` with the following content:

.overlays/quickstart/kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: inetsoft
bases:
  - ../../cluster
secretGenerator:
  - name: inetsoft
    behavior: merge
    envs:
      - secrets.properties
----

Override the master password, server license key, and Spark license key secrets with a unique password, your server license key, and your Spark license key, respectively. Put these in the `overlays/quickstart/secrets.properties` file. If you do not have a Spark license key, omit the `INETSOFT_SPARK_LICENSE` property.

.overlays/quickstart/secrets.properties
[source,properties]
----
INETSOFT_MASTER_PASSWORD=a_strong_password
INETSOFT_LICENSE=your_server_license_key
INETSOFT_SPARK_LICENSE=your_spark_license_key
----

[[quickstart-kubernetes-cluster-pv]]
===== Persistent Volume Provisioning

As configured, the example uses the `standard` storage class for Minikube. If your cluster uses a different storage class for dynamic provisioning, you'll need to override this. For example, if you are using a NFS provisioner with the storage class `nfs-client`, create a file named `overlays/quickstart/pvc-mysql.yaml` with the following content:

.overlays/quickstart/pvc-mysql.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: inetsoft-mysql-pvc
spec:
  storageClassName: nfs-client
----

Create a file named `overlays/quickstart/pvc-shared.yaml` with the following content:

.overlays/quickstart/pvc-shared.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: inetsoft-shared-pvc
spec:
  storageClassName: nfs-client
----

Create a file named `overlays/quickstart/pvc-spark-master.yaml` with the following content:

.overlays/quickstart/pvc-spark-master.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: inetsoft-spark-master-pvc
spec:
  storageClassName: nfs-client
----

Create a file named `overlays/quickstart/statefulset-spark-worker.yaml` with the following content:

.overlays/quickstart/statefulset-spark-worker.yaml
[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: inetsoft-spark-worker
spec:
  volumeClaimTemplates:
    - metadata:
        name: hadoop-data
      spec:
        storageClassName: nfs-client
----

Then, append the following to `overlays/quickstart/kustomization.yaml`:

.overlays/quickstart/kustomization.yaml
[source,yaml]
----
patchesStrategicMerge:
  - pvc-mysql.yaml
  - pvc-shared.yaml
  - pvc-spark-master.yaml
  - statefulset-spark-worker.yaml
----

[[quickstart-kubernetes-cluster-spark]]
===== Spark

If you don't have a Spark license key, change the number of replicas for the Spark worker stateful set to one. Create or edit the `overlays/quickstart/statefulset-spark-worker.yaml` file with the following content:

.overlays/quickstart/statefulset-spark-worker.yaml
[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: inetsoft-spark-worker
spec:
  replicas: 1
----

Then add the following to `overlays/quickstart/kustomization.yaml` if you have not already:

.overlays/quickstart/kustomization.yaml
[source,yaml]
----
patchesStrategicMerge:
  - statefulset-spark-worker.yaml
----

[[quickstart-kubernetes-cluster-ingress]]
===== Ingress

By default, the ingress is mapped to the `inetsoft.your-company.com` host. If you want to change this, create a file named `overlays/quickstart/config.properties` with the following content:

.overlays/quickstart/config.properties
[source,properties]
----
INETSOFT_PROXY_URL=http://your_host_name
----

Then create a file named `overlays/quickstart/ingress.yaml` with the following content:

.overlays/quickstart/ingress.yaml
[source,yaml]
----
- op: replace
  path: /spec/rules/0/host
  value: your_hostname
----

Edit the `overlays/quickstart/kustomization.yaml` file and append the following:

.overlays/quickstart/kustomization.yaml
[source,yaml]
----
configMapGenerator:
  - name: inetsoft
    behavior: merge
    envs:
      - config.properties
patches:
  - target:
      kind: Ingress
      name: inetsoft-ingress
    path: ingress-patch.yaml
----

[[quickstart-kubernetes-cluster-deploy]]
===== Deployment

Deploy the application to Kubernetes by running:

[source,shell]
----
kubectl apply -k overlays/quickstart
----

Remove the application by running:

[source,shell]
----
kubectl delete -k overlays/quickstart
----

[[additional-docs]]
== Additional Documentation

* link:docs/docker-images.adoc[Docker Images]
* link:docs/configuration.adoc[Configuration]
* link:docs/spark.adoc[Spark]
* link:docs/production.adoc[Production Deployment]
